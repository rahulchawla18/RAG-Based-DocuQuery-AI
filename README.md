# ğŸ§  DocuQuery AI â€” PDF Question Answering with RAG

DocuQuery AI is a **Retrieval-Augmented Generation (RAG)** system built with **Streamlit**, allowing users to upload **PDF files**, process them into semantically meaningful chunks, and interactively query them using **OpenAI** or **Ollama** LLMs. The project leverages **ChromaDB** for vector storage and supports multiple **embedding models** and **LLMs** for flexible experimentation.

> _â€œAsk questions directly from your uploaded PDFs â€” like ChatGPT, but with your own documents.â€_

## ğŸ”§ Features

âœ… Upload and parse PDF documents  
âœ… Split documents into overlapping semantic chunks  
âœ… Choose between multiple Embedding & LLM models  
âœ… Store vector embeddings in ChromaDB  
âœ… Query documents using natural language  
âœ… Generate context-aware responses using RAG  
âœ… Streamlit-based responsive UI

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/rahulchawla18/RAG-Based-DocuQuery-AI.git
cd RAG-BASED-DOCUQUERY-AI
```

### 2. Create a virtual environment

python -m venv venv
source venv/bin/activate        # On Windows: venv\Scripts\activate

### 3. Install dependencies

pip install -r requirements.txt

### 4. Set your environment variables

Create a .env file in the root:
    OPENAI_API_KEY=your-openai-api-key

If using Ollama locally for embeddings or LLMs, make sure Ollama is running.

### 5. Run the app

streamlit run app.py